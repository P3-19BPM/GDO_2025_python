from dotenv import load_dotenv
from openpyxl import load_workbook
from shapely.geometry import Point
import os
import pyodbc
import geopandas as gpd
import pandas as pd

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# --- Configura√ß√£o de conex√£o ODBC ---
dsn_name = 'Sample Cloudera Impala DSN'
username = os.getenv('DB_USERNAME')
password = os.getenv('DB_PASSWORD')

# --- Caminhos Comuns ---
# Ajuste estes caminhos para o seu ambiente local
geojson_path = r"E:\QGis\csv\Mapas_Tratados\SubSetores_19BPM_GeoJSON.json"
sql_scripts_folder = r"E:\GitHub\GDO_2025_python\sql_scripts"

# --- Configura√ß√µes para GDO_2025 ---
gdo_output_excel_path = r"E:\GDO\GDO_2025\Monitoramento_GDO_2025.xlsx"
gdo_sql_files_mapping = {
    "BD_IMV2025": "BD_IMV2025.sql",
    "BD_ICVPe": "BD_ICVPe.sql",
    "BD_ICVPa": "BD_ICVPa.sql",
    "BD_POG": "BD_POG.sql",
    "BD_PL": "BD_PL.sql",
    "BD_IRTD": "BD_IRTD.sql"
}

# --- Configura√ß√µes para INT_Comunitaria_2025 ---
int_cum_output_excel_path = r"E:\GDO\GDO_2025\Monitoramento_INT_Comunitaria_2025.xlsx"
int_cum_sql_files_mapping = {
    "BD_MRPP_INT_CUM": "BD_MRPP_INT_CUM.sql",
    "BD_RC_INT_CUM": "BD_RC_INT_CUM.sql",
    "BD_VCP_INT_CUM": "BD_VCP_INT_CUM.sql",
    "BD_VTC_DENOMINADOR_INT_CUM": "BD_VTC_DENOMINADOR_INT_CUM.sql",
    "BD_VTC_NUMERADOR_INT_CUM": "BD_VTC_NUMERADOR_INT_CUM.sql",
    "BD_VT_DENOMINADOR_INT_CUM": "BD_VT_DENOMINADOR_INT_CUM.sql",
    "BD_VT_NUMERADOR_INT_CUM": "BD_VT_NUMERADOR_INT_CUM.sql"
}

# --- Caminhos para CSVs de GDO ---
gdo_csv_paths = {
    "BD_IMV2025": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\GDO_2025_1.csv",
    "BD_ICVPe": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\GDO_2025_2.csv",
    "BD_ICVPa": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\GDO_2025_3.csv",
    "BD_POG": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\GDO_2025_4.csv",
    "BD_PL": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\GDO_2025_5.csv",
    "BD_IRTD": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\GDO_2025_6.csv",
}

# --- Caminhos para CSVs de INT_Comunitaria ---
int_cum_csv_paths = {
    "BD_MRPP_INT_CUM": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\INT_COM_2025_1.csv",
    "BD_RC_INT_CUM": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\INT_COM_2025_2.csv",
    "BD_VCP_INT_CUM": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\INT_COM_2025_3.csv",
    "BD_VTC_DENOMINADOR_INT_CUM": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\INT_COM_2025_4.csv",
    "BD_VTC_NUMERADOR_INT_CUM": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\INT_COM_2025_5.csv",
    "BD_VT_DENOMINADOR_INT_CUM": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\INT_COM_2025_6.csv",
    "BD_VT_NUMERADOR_INT_CUM": r"E:\Painel_PowerBI\BASE_GDO\BD_2025\INT_COM_2025_7.csv"
}


# --- Fun√ß√£o para ler o conte√∫do de um arquivo SQL ---
def read_sql_file(filepath):
    if not os.path.exists(filepath):
        print(f"‚ùå Erro: Arquivo SQL n√£o encontrado em {filepath}")
        return None
    with open(filepath, 'r', encoding='utf-8') as file:
        return file.read()

# --- Fun√ß√£o para processar cada consulta SQL e retornar um DataFrame ---


def fetch_data_from_impala(sql_query, dsn, user, pwd):
    conn = None
    cursor = None
    try:
        connection_string = f'DSN={dsn};UID={user};PWD={pwd};'
        conn = pyodbc.connect(connection_string, autocommit=True)
        cursor = conn.cursor()
        print(f"‚è≥ Executando consulta SQL...")
        cursor.execute(sql_query)

        if cursor.description is None:
            print("‚ö†Ô∏è ATEN√á√ÉO: cursor.description √© None. A consulta pode n√£o ter retornado resultados ou falhou no servidor.")
            print(f"Consulta SQL executada: \n{sql_query[:200]}...")
            try:
                rows = cursor.fetchall()
                if not rows:
                    print("‚ö†Ô∏è Nenhuma linha retornada pela consulta.")
                else:
                    print(
                        f"‚ö†Ô∏è Linhas retornadas, mas sem descri√ß√£o de coluna: {len(rows)} linhas.")
            except Exception as fetch_e:
                print(
                    f"‚ùå Erro ao tentar buscar linhas ap√≥s cursor.description ser None: {fetch_e}")
            raise ValueError(
                "N√£o foi poss√≠vel obter a descri√ß√£o das colunas da consulta SQL.")

        columns = [desc[0] for desc in cursor.description]
        rows = cursor.fetchall()
        df = pd.DataFrame.from_records(rows, columns=columns)
        print(f"‚úÖ Consulta finalizada: {df.shape[0]} registros obtidos.")
        return df
    except Exception as e:
        print(f"‚ùå Erro ao conectar ou executar consulta no Impala: {e}")
        raise
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()

# --- Fun√ß√£o para Processar Cada DataFrame e realizar a jun√ß√£o espacial ---


def process_dataframe_for_spatial_join(df, geojson_path):
    try:
        # 1. Tratamento de coordenadas nulas e convers√£o para num√©rico
        # Cria uma c√≥pia para evitar SettingWithCopyWarning
        df_processed = df.copy()

        # Converte as colunas de latitude e longitude para tipo num√©rico
        # errors='coerce' ir√° transformar valores n√£o num√©ricos em NaN
        df_processed['numero_latitude'] = pd.to_numeric(
            df_processed['numero_latitude'], errors='coerce')
        df_processed['numero_longitude'] = pd.to_numeric(
            df_processed['numero_longitude'], errors='coerce')

        # Remove linhas onde as coordenadas s√£o NaN ap√≥s a convers√£o
        initial_rows = len(df_processed)
        df_processed.dropna(
            subset=['numero_latitude', 'numero_longitude'], inplace=True)
        if len(df_processed) < initial_rows:
            print(
                f"‚ö†Ô∏è {initial_rows - len(df_processed)} linhas removidas devido a coordenadas nulas ou inv√°lidas.")

        # 2. Tratamento de REDS duplicados (mant√©m a primeira ocorr√™ncia)
        # Assumindo que 'numero_ocorrencia' √© a coluna que identifica o REDS
        if 'numero_ocorrencia' in df_processed.columns:
            initial_rows_after_na = len(df_processed)
            df_processed.drop_duplicates(
                subset=['numero_ocorrencia'], inplace=True)
            if len(df_processed) < initial_rows_after_na:
                print(
                    f"‚ö†Ô∏è {initial_rows_after_na - len(df_processed)} REDS duplicados removidos (mantida a primeira ocorr√™ncia).")
        else:
            print(
                "‚ö†Ô∏è Coluna 'numero_ocorrencia' n√£o encontrada para verificar duplicados.")

        # Verificar se as colunas necess√°rias existem ap√≥s os tratamentos
        required_columns = ['numero_longitude', 'numero_latitude']
        missing_columns = [
            col for col in required_columns if col not in df_processed.columns]

        if missing_columns:
            print(
                f"\n[ERRO] DataFrame n√£o possui as colunas esperadas: {required_columns} ap√≥s tratamento.")
            print(f"Colunas faltando: {missing_columns}")
            print(f"Colunas dispon√≠veis: {df_processed.columns.tolist()}\n")
            raise ValueError(f"Colunas faltando: {missing_columns}")

        # Criar GeoDataFrame
        # Garante que Point receba valores escalares float
        df_processed['geometry'] = df_processed.apply(lambda row: Point(
            float(row['numero_longitude']), float(row['numero_latitude'])), axis=1)
        points_gdf = gpd.GeoDataFrame(
            df_processed, geometry='geometry', crs='EPSG:4326')

        # Carregar e alinhar o GeoJSON
        polygons_gdf = gpd.read_file(geojson_path)
        if polygons_gdf.crs != 'EPSG:4326':
            polygons_gdf = polygons_gdf.to_crs('EPSG:4326')

        # Jun√ß√£o espacial
        print("üìç Realizando jun√ß√£o espacial...")
        result_gdf = gpd.sjoin(points_gdf, polygons_gdf,
                               how='left', predicate='within')

        # Limpar colunas desnecess√°rias (como visuais do GeoJSON)
        colunas_a_remover = ['index_right',
                             'fill-opacity', 'stroke-opacity', 'stroke']
        result_gdf.drop(columns=[
            col for col in colunas_a_remover if col in result_gdf.columns], inplace=True)

        # Assegurar que as colunas 'name', 'PELOTAO', 'CIA_PM' existam antes de tentar acess√°-las
        # Se elas n√£o existirem no resultado da jun√ß√£o, elas ser√£o adicionadas como NaN
        for col in ['name', 'PELOTAO', 'CIA_PM']:
            if col not in result_gdf.columns:
                result_gdf[col] = None

        # Selecionar as colunas originais do DataFrame processado mais as novas colunas da jun√ß√£o
        # Garantir que as colunas de sa√≠da existam no result_gdf antes de selecion√°-las
        output_columns = [col for col in df_processed.columns if col !=
                          'geometry'] + ['name', 'PELOTAO', 'CIA_PM']
        output_columns = [
            col for col in output_columns if col in result_gdf.columns]
        result = result_gdf[output_columns]

        result = result.rename(columns={
            'name': 'SubSetor',
            'PELOTAO': 'Pelotao',
            'CIA_PM': 'CIA_PM'
        })

        if 'data_hora_fato' in result.columns:
            result['data_fato'] = pd.to_datetime(
                result['data_hora_fato'], errors='coerce').dt.date

        return result

    except Exception as e:
        print(
            f"\n[ERRO] Falha ao processar o DataFrame para jun√ß√£o espacial: {str(e)}\n")
        raise

# --- Fun√ß√£o para processar um conjunto de dados e salvar no Excel ---


def process_data_set(output_excel_path, sql_mapping, geojson_path, dsn, user, pwd, dataset_name="", csv_paths=None):
    print(f"\n{'='*50}")
    print(f"Iniciando processamento para: {dataset_name}")
    print(f"{'='*50}\n")

    # Cria o diret√≥rio para os CSVs se ele n√£o existir
    if csv_paths:
        # Pega o diret√≥rio do primeiro CSV para criar a pasta base
        # Garante que o dicion√°rio n√£o est√° vazio antes de tentar acessar
        if csv_paths:
            first_csv_path = list(csv_paths.values())[0]
            csv_dir = os.path.dirname(first_csv_path)
            os.makedirs(csv_dir, exist_ok=True)
            print(f"DEBUG: Diret√≥rio para CSVs criado/verificado: {csv_dir}")

    try:
        # Use pd.ExcelWriter com mode='a' para anexar/substituir abas
        # Se o arquivo n√£o existir, ele ser√° criado automaticamente
        with pd.ExcelWriter(output_excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
            for sheet_name, sql_filename in sql_mapping.items():
                sql_filepath = os.path.join(sql_scripts_folder, sql_filename)
                print(
                    f"\nProcessando aba: {sheet_name} a partir de {sql_filepath}")

                sql_query = read_sql_file(sql_filepath)
                if sql_query is None:
                    print(
                        f"‚ö†Ô∏è Pulando {sheet_name} devido a arquivo SQL n√£o encontrado ou vazio.")
                    continue

                try:
                    raw_data_df = fetch_data_from_impala(
                        sql_query, dsn, user, pwd)

                    # --- SALVAR RAW DATA EM CSV (ANTES DO PROCESSAMENTO GEOESPACIAL) ---
                    if csv_paths and sheet_name in csv_paths:
                        csv_output_path = csv_paths[sheet_name]
                        raw_data_df.to_csv(
                            csv_output_path, index=False, encoding='utf-8')
                        print(
                            f"‚úÖ Dados brutos salvos em CSV: {csv_output_path}")
                    # --- FIM SALVAR RAW DATA EM CSV ---

                    processed_data = process_dataframe_for_spatial_join(
                        raw_data_df, geojson_path)
                    processed_data.to_excel(
                        writer, sheet_name=sheet_name, index=False)
                except Exception as e:
                    print(f"‚ùå Erro ao processar {sheet_name}: {e}")
                    # Continua para a pr√≥xima aba mesmo se uma falhar
                    continue
        print(
            f"\n‚úÖ Arquivo {dataset_name} atualizado com sucesso em: {output_excel_path}")
    except FileNotFoundError:
        # Se o arquivo n√£o existir, crie um novo com mode='w'
        with pd.ExcelWriter(output_excel_path, engine='openpyxl', mode='w') as writer:
            for sheet_name, sql_filename in sql_mapping.items():
                sql_filepath = os.path.join(sql_scripts_folder, sql_filename)
                print(
                    f"\nProcessando aba: {sheet_name} a partir de {sql_filepath}")

                sql_query = read_sql_file(sql_filepath)
                if sql_query is None:
                    print(
                        f"‚ö†Ô∏è Pulando {sheet_name} devido a arquivo SQL n√£o encontrado ou vazio.")
                    continue

                try:
                    raw_data_df = fetch_data_from_impala(
                        sql_query, dsn, user, pwd)

                    # --- SALVAR RAW DATA EM CSV (ANTES DO PROCESSAMENTO GEOESPACIAL) ---
                    if csv_paths and sheet_name in csv_paths:
                        csv_output_path = csv_paths[sheet_name]
                        raw_data_df.to_csv(
                            csv_output_path, index=False, encoding='utf-8')
                        print(
                            f"‚úÖ Dados brutos salvos em CSV: {csv_output_path}")
                    # --- FIM SALVAR RAW DATA EM CSV ---

                    processed_data = process_dataframe_for_spatial_join(
                        raw_data_df, geojson_path)
                    processed_data.to_excel(
                        writer, sheet_name=sheet_name, index=False)
                except Exception as e:
                    print(f"‚ùå Erro ao processar {sheet_name}: {e}")
                    continue
        print(
            f"\n‚úÖ Novo arquivo {dataset_name} criado em: {output_excel_path}")
    except Exception as e:
        print(
            f"\n‚ùå [ERRO FATAL] Ocorreu um erro durante o processamento de {dataset_name}: {e}")


# --- Execu√ß√£o Principal ---
if __name__ == "__main__":
    # Processar dados GDO
    process_data_set(
        output_excel_path=gdo_output_excel_path,
        sql_mapping=gdo_sql_files_mapping,
        geojson_path=geojson_path,
        dsn=dsn_name,
        user=username,
        pwd=password,
        dataset_name="GDO_2025",
        csv_paths=gdo_csv_paths  # Adicionado
    )

    # Processar dados de Intera√ß√£o Comunit√°ria
    process_data_set(
        output_excel_path=int_cum_output_excel_path,
        sql_mapping=int_cum_sql_files_mapping,
        geojson_path=geojson_path,
        dsn=dsn_name,
        user=username,
        pwd=password,
        dataset_name="INT_Comunitaria_2025",
        csv_paths=int_cum_csv_paths  # Adicionado
    )

    print("\nProcessamento de todos os conjuntos de dados conclu√≠do.")
